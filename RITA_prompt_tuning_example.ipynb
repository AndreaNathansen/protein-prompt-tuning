{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH84sRgJ-6X0"
      },
      "source": [
        "# Example notebook for tuning a prompt for RITA on protein family PF03272."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRz1VIqQ_l2F"
      },
      "outputs": [],
      "source": [
        "#@title Setup for Colab only\n",
        "import os\n",
        "colab = 'google.colab' in str(get_ipython())\n",
        "if colab:\n",
        "  colab_prefix = \"/content/drive/MyDrive/\"\n",
        "  !pip install transformers==4.20.1\n",
        "  !pip install biopython\n",
        "  !pip install git+https://github.com/AndreaNathansen/protein-prompt-tuning.git#egg=protein-prompt-tuning --log PIP_LOG\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  # Download datasets\n",
        "  if not os.path.exists(colab_prefix + \"datasets\"):\n",
        "    os.makedirs(colab_prefix + \"datasets\") \n",
        "  !wget -N https://raw.githubusercontent.com/AndreaNathansen/protein-prompt-tuning/main/datasets/InterProUniprotPF03272prepared_train.fasta -P /content/drive/MyDrive/datasets/\n",
        "  !wget -N https://raw.githubusercontent.com/AndreaNathansen/protein-prompt-tuning/main/datasets/InterProUniprotPF03272prepared_validation.fasta -P /content/drive/MyDrive/datasets/\n",
        "  !wget -N https://raw.githubusercontent.com/AndreaNathansen/protein-prompt-tuning/main/datasets/InterProUniprotPF03272prepared_test.fasta -P /content/drive/MyDrive/datasets/\n",
        "else:\n",
        "  colab_prefix=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wfIHPSJ-6X1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from mkultra.evaluator import Evaluator\n",
        "import mkultra.sequence_loader as sequence_loader\n",
        "from mkultra.trainers import SoftPromptTrainer\n",
        "from mkultra.tuning import RITAPromptTuningLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxwMlD_Q-6X3"
      },
      "source": [
        "## Set up training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6II9h0Q-6X5"
      },
      "outputs": [],
      "source": [
        "seed = 1234567890"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Vcozbn-6X6"
      },
      "outputs": [],
      "source": [
        "sp_name = \"RITA-prompt-tuning-example\"\n",
        "# Specify the project directory base.\n",
        "project_dir = f\"{colab_prefix}soft_prompts/{sp_name}/\"\n",
        "\n",
        "if not os.path.exists(project_dir):\n",
        "    os.makedirs(project_dir)\n",
        "\n",
        "model_name = \"lightonai/RITA_s\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIEFgnBH-6X7"
      },
      "outputs": [],
      "source": [
        "n_tokens = 10\n",
        "block_size = 1014 # 1024 - 10 (prompt size)\n",
        "batch_size = 2\n",
        "optimizer_params = {\"lr\": 0.001}\n",
        "num_epochs = 2\n",
        "checkpoint_interval = 1\n",
        "patience = 2\n",
        "init_from_vocab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjtXu5ML-6X8"
      },
      "outputs": [],
      "source": [
        "dataset_file_train = colab_prefix + \"datasets/InterProUniprotPF03272prepared_train.fasta\"\n",
        "dataset_file_validation = colab_prefix + \"datasets/InterProUniprotPF03272prepared_validation.fasta\"\n",
        "dataset_file_test = colab_prefix + \"datasets/InterProUniprotPF03272prepared_test.fasta\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2ScfgZ4-6X8"
      },
      "outputs": [],
      "source": [
        "model = RITAPromptTuningLM.from_pretrained(model_name).half().to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAhoQXFS-6X9"
      },
      "outputs": [],
      "source": [
        "dataset = sequence_loader.FastaDataset(dataset_file_train, tokenizer, block_size, tokenizer.vocab['<PAD>'], tokenizer.vocab['<EOS>'], tokenizer.vocab['<EOS>'])\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataset_val = sequence_loader.FastaDataset(dataset_file_validation, tokenizer, block_size, tokenizer.vocab['<PAD>'], tokenizer.vocab['<EOS>'], tokenizer.vocab['<EOS>'])\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNIxnHWh-6YA"
      },
      "outputs": [],
      "source": [
        "trainer = SoftPromptTrainer(\n",
        "    model=model,\n",
        "    optimizer_class=AdamW,\n",
        "    optimizer_params=optimizer_params,\n",
        "    project_dir=project_dir,\n",
        "    data_loader_train=dataloader,\n",
        "    data_loader_eval=dataloader_val,\n",
        "    checkpoint_interval=checkpoint_interval,\n",
        "    patience=patience,\n",
        "    n_tokens=n_tokens,\n",
        "    shuffle_seed=seed,\n",
        "    init_from_vocab=init_from_vocab,\n",
        "    prompt_init_seed=seed)\n",
        "\n",
        "trainer.train(num_epochs=num_epochs)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjxMsPyJ-6YB"
      },
      "source": [
        "## Evaluate trained prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohtN302C-6YB"
      },
      "outputs": [],
      "source": [
        "dataset_test = sequence_loader.FastaDataset(dataset_file_test, tokenizer, block_size, tokenizer.vocab['<PAD>'], tokenizer.vocab['<EOS>'], tokenizer.vocab['<EOS>'])\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1oNOwCT-6YC"
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator(\n",
        "        model=model,\n",
        "        is_prompt_tuned=True,\n",
        "        data_loader_test=dataloader_test,\n",
        "        project_dir=project_dir)\n",
        "perplexity = evaluator.evaluate_perplexity()\n",
        "print(perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEsk0sGH-6YC"
      },
      "source": [
        "Compare to base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHDIa5XX-6YD"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).half().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgEFJTZG-6YD"
      },
      "outputs": [],
      "source": [
        "base_evaluator = Evaluator(\n",
        "        model=base_model,\n",
        "        is_prompt_tuned=False,\n",
        "        data_loader_test=dataloader_test)\n",
        "base_perplexity = base_evaluator.evaluate_perplexity()\n",
        "print(base_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZn9GlJW-6YE"
      },
      "source": [
        "## Generate sequences with the prompt-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-ZvvI1F-6YF"
      },
      "outputs": [],
      "source": [
        "# leave out the EOS token that the RITA tokenizer always appends\n",
        "input_ids = tokenizer(\"<EOS>\", return_tensors=\"pt\").input_ids[:, :-1].to(\"cuda\")\n",
        "output = model.generate(input_ids=input_ids, max_length=block_size, do_sample=True, top_k=950, repetition_penalty=1.2, \n",
        "                    num_return_sequences=2, eos_token_id=2)\n",
        "sequences = [tokenizer.decode(output_ids) for output_ids in output]                   \n",
        "print([sequence.replace('<EOS>','').replace(' ', '') for j, sequence in enumerate(sequences)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y74jN45j-6YF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
