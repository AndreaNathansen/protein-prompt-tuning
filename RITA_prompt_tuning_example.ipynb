{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook for tuning a prompt for RITA on protein family PF03272."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from mkultra.evaluator import Evaluator\n",
    "import mkultra.sequence_loader as sequence_loader\n",
    "from mkultra.trainers import SoftPromptTrainer\n",
    "from mkultra.tuning import RITAPromptTuningLM\n",
    "from utils.train_utils import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1234567890\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sp_name = \"RITA-prompt-tuning-example\"\n",
    "# Specify the project directory base.\n",
    "project_dir = f\"soft_prompts/{sp_name}/\"\n",
    "\n",
    "if not os.path.exists(project_dir):\n",
    "    os.makedirs(project_dir)\n",
    "\n",
    "model_name = \"lightonai/RITA_s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n_tokens = 10\n",
    "block_size = 1014 # 1024 - 10 (prompt size)\n",
    "batch_size = 2\n",
    "optimizer_params = {\"lr\": 0.001}\n",
    "num_epochs = 2\n",
    "checkpoint_interval = 1\n",
    "patience = 2\n",
    "init_from_vocab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset_file_train = \"datasets/InterProUniprotPF03272prepared_train.fasta\"\n",
    "dataset_file_validation = \"datasets/InterProUniprotPF03272prepared_validation.fasta\"\n",
    "dataset_file_test = \"datasets/InterProUniprotPF03272prepared_test.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = RITAPromptTuningLM.from_pretrained(model_name).half().to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = sequence_loader.FastaDataset(dataset_file_train, tokenizer, block_size, tokenizer.vocab['<PAD>'])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_val = sequence_loader.FastaDataset(dataset_file_validation, tokenizer, block_size, tokenizer.vocab['<PAD>'])\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SoftPromptTrainer(\n",
    "    model=model,\n",
    "    optimizer_class=AdamW,\n",
    "    optimizer_params=optimizer_params,\n",
    "    project_dir=project_dir,\n",
    "    data_loader_train=dataloader,\n",
    "    data_loader_eval=dataloader_val,\n",
    "    checkpoint_interval=checkpoint_interval,\n",
    "    patience=patience,\n",
    "    n_tokens=n_tokens,\n",
    "    shuffle_seed=seed,\n",
    "    init_from_vocab=init_from_vocab,\n",
    "    prompt_init_seed=seed)\n",
    "\n",
    "trainer.train(num_epochs=num_epochs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate trained prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = sequence_loader.FastaDataset(dataset_file_test, tokenizer, block_size, tokenizer.vocab['<PAD>'])\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\n",
    "        model=model,\n",
    "        is_prompt_tuned=True,\n",
    "        data_loader_test=dataloader_test,\n",
    "        project_dir=project_dir)\n",
    "perplexity = evaluator.evaluate_perplexity()\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).half().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_evaluator = Evaluator(\n",
    "        model=base_model,\n",
    "        is_prompt_tuned=False,\n",
    "        data_loader_test=dataloader_test)\n",
    "base_perplexity = base_evaluator.evaluate_perplexity()\n",
    "print(base_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequences with the prompt-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# leave out the EOS token that the RITA tokenizer always appends\n",
    "input_ids = tokenizer(\"<EOS>\", return_tensors=\"pt\").input_ids[:, :-1].to(\"cuda\")\n",
    "output = model.generate(input_ids=input_ids, max_length=block_size, do_sample=True, top_k=950, repetition_penalty=1.2, \n",
    "                    num_return_sequences=2, eos_token_id=2)\n",
    "sequences = [tokenizer.decode(output_ids) for output_ids in output]                   \n",
    "print([sequence.replace('<EOS>','').replace(' ', '') for j, sequence in enumerate(sequences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
